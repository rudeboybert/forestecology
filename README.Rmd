---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  fig.width = 16/2,
  fig.height = 9/2,
  message = FALSE,
  warning = FALSE,
  cache = TRUE
)
```

# forestecology 

[![Travis Build Status](https://travis-ci.org/rudeboybert/forestecology.svg?branch=master)](https://travis-ci.org/rudeboybert/forestecology)
[![Lifecycle: stable](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![CRAN status](https://www.r-pkg.org/badges/version/forestecology)](https://cran.r-project.org/package=forestecology)



## Installation

You can install the released version of forestecology from [CRAN](https://CRAN.R-project.org) with:

```{r, eval = FALSE}
install.packages("forestecology")
```

And the development version from [GitHub](https://github.com/) with:

```{r, eval = FALSE}
# install.packages("remotes")
# remotes::install_github("rudeboybert/forestecology")
```



## Example analysis


We present an example analysis using:

* [Michigan Big Woods](https://doi.org/10.7302/wx55-kt18) research plot data
* [Smithsonian Conservation Biology Institute (SCBI) ForestGEO](https://github.com/SCBI-ForestGEO/SCBI-ForestGEO-Data/tree/master/tree_main_census) research plot data

```{r}
library(tidyverse)
library(forestecology)
library(snakecase)
library(skimr)
library(sf)
library(sfheaders)
# devtools::install_github("rvalavi/blockCV")
library(blockCV)
library(tictoc)
library(yardstick)
library(viridis)
library(lubridate)

run_scbi <- TRUE
run_bw <- FALSE
```



### Load & preprocess data

The data for the Big Woods plor are included in the package. For SCBI, we load and preprocess the data corresponding to the two census reflecting the time period over which we consider growth in dbh. We load them into R as "tibble" data frames thereby ensuring a standardized input/output format that can be used across all `tidyverse` packages @tidyverse. Furthermore, we ensure that the different variables have the correct names, types (`dbl`, `data`, `factor`). 

**Big Woods**:

```{r, eval = run_bw}
# Big Woods
# Read in census data from 2008 & 2014
data(bw_census_2008, bw_census_2014, bw_species)

# Append additional species data
bw_census_2008 <- bw_census_2008 %>%
  left_join(bw_species, by = "sp") %>%
  select(-c(genus, species, latin))

```

**SCBI**:

```{r, eval = run_scbi}
# SCBI
scbi_2013 <- 
  "https://github.com/SCBI-ForestGEO/SCBI-ForestGEO-Data/raw/master/tree_main_census/data/census-csv-files/scbi.stem2.csv" %>% 
  read_csv() %>% 
  select(
    treeID, stemID, sp, quadrat, gx, gy, dbh, 
    date = ExactDate, codes, status
  ) %>%
  mutate(date = mdy(date))

scbi_2018 <-
  "https://github.com/SCBI-ForestGEO/SCBI-ForestGEO-Data/raw/master/tree_main_census/data/census-csv-files/scbi.stem3.csv" %>% 
  read_csv() %>% 
  select(
    treeID, stemID, sp, quadrat, gx, gy, dbh, 
    date = ExactDate, codes, status
  ) %>% 
  mutate(dbh = as.numeric(dbh),
         date = mdy(date)) 
```



### Compute growth

We then combine the two census data frames into a single data frame  that now includes a numerical variable `growth` reflecting the average annual growth of dbh in cm. Furthermore, variables that (in theory) remain unchanged between censuses appear only once, such as `gx`, `gy`, species-related variables. Variables that should change between censuses are tagged with `1/2` indicating earlier/later, such as `dbh1/dbh2`, `codes1/codes2`.

The resulting data frames are named with some variation of `growth_df`.

**Big Woods**:

```{r, eval = run_bw}
# Big Woods
id <- "treeID"

# we need to filter out the resprouts
bw_census_2014 <- bw_census_2014 %>% 
  filter(!str_detect(codes, 'R'))

bw_growth_df <-
  # Merge both censuses and compute growth:
  compute_growth(bw_census_2008, bw_census_2014, id) %>%
  mutate(
    sp = to_any_case(sp),
    sp = as.factor(sp),
    species = sp,
    family = as.factor(family),
    trait_group = as.factor(trait_group)) %>%
  # drop stemID
  select(-stemID)

```

**SCBI**:

```{r, eval = run_scbi}
# SCBI
census_df1 <- scbi_2013 
census_df2 <- scbi_2018
id <- "stemID"

scbi_growth_df <- 
  # Merge both censuses and compute growth:
  compute_growth(census_df1, census_df2, id) %>%
  # they are mesuaring in mm while we are measuring in cm!!!
  mutate(growth = growth/10)

```

**Comparison**:

```{r, eval = run_scbi & run_bw}
# Both Big Woods & SCBI
growth_df <- bind_rows(
  bw_growth_df %>% st_drop_geometry %>% select(growth) %>% mutate(site = "bw"),
  scbi_growth_df %>% st_drop_geometry %>% select(growth) %>% mutate(site = "scbi")
)
ggplot(growth_df, aes(x = growth, y = ..density.., fill = site)) +
  geom_histogram(alpha = 0.5, position = "identity", binwidth = 0.05) +
  labs(x = "Average annual growth in dbh (cm per yr)") +
  coord_cartesian(xlim = c(-0.5, 1))
```



### Add spatial information

We now add spatial information to the `growth_df` data frame. Specifically:

1. In order to control for study region "edge effects" (cite Waller?), we need a function that adds "buffers" of trees. In our case, since our model of interspecies competition relies on a spatial definition of who is a "neighboring competitor" and certain explanatory varibles such as biomass are cummulative, we need to ensure that all trees being modeled are not biased to have different neighbor structure, in particular the trees at the boundary of study regions.
1. Assign each tree to a "fold" for cross-validation purposes. Conventional cross-validation schemes assign observations to folds by resampling individual observations at random. However, underlying this scheme is an assumption that the observations are independent. In the case of forest census data, observations exhibit spatial autocorrelation, and thus this dependence must be incorporated in our resampling scheme @roberts2017 @pohjankukka2017. Packages that have implemented spatial cross-validation include @valavi2019

Thus, before we can incorporate the above information to `growth_df`, we need to define two constants:

```{r}
cv_fold_size <- 100
max_dist <- 7.5
```


#### Defining buffers

For a focal tree of interest, the `max_dist` of `r max_dist` meters acts as a radius defining a neighborhood within which all trees are considered competitors. In other words, all trees within `r max_dist` meters of the focal tree are considered competitor trees. Other studies have estimated this distance; we used `r max_dist` meters as an average of estimated values [@canham2004, @canham2006, @uriarte2004, @tatsumi2013]. Following Tobler's first law of geography that "everything is related to everything else, but near things are more related than distant things." @tobler1970firstlawofgeo, we assume that the degree of spatial autocorrelation is inversely-related to distance. However, we further assume that once trees are more than `r max_dist` meters apart, this autocorrelation is negligeable. 

We use the function `add_buffer_variable` to identifiy which trees are inside of a buffer region of size `max_dist`. The user will need to specify a study region and convert to a `sf_polygon`.

**Example of buffer at work**:

```{r}
# Boundary polygon
square_boundary <- tibble(
  x = c(0,0,1,1),
  y = c(0,1,1,0)
) %>% 
  sf_polygon()

# "Trees" in polygon
trees_df <- tibble(
    x = runif(100),
    y = runif(100)
  ) %>%
  sf_point()

# Buffer polygon
trees_df <- trees_df %>%
  add_buffer_variable(direction = "in", size = 0.1, region = square_boundary)

ggplot() +
  geom_sf(data = trees_df, aes(col = buffer))


```

**Big Woods**:

```{r, eval = run_bw}
# Bigwoods
data(bw_study_region)

# Add buffer variable to data frame
bw_growth_df <- bw_growth_df %>%
  add_buffer_variable(direction = "in", size = max_dist, region = bw_study_region)

ggplot() +
  geom_sf(data = bw_growth_df, aes(col = buffer))

```

**SCBI**:

```{r, eval = run_scbi}
# SCBI
# Study region boundary polygon
scbi_study_region <- tibble(x = c(0,400,400,0,0), y = c(0,0,640,640,0)) %>% 
  sf_polygon()

# Add buffer variable to data frame
scbi_growth_df <- scbi_growth_df %>%
  add_buffer_variable(direction = "in", size = max_dist, region = scbi_study_region)

ggplot() +
  geom_sf(data = scbi_growth_df, aes(col = buffer))
```


#### Defining spatial cross-validation folds

We use the [`blockCV`](https://github.com/rvalavi/blockCV) package to define the spatial grid, whose elements will act as the folds in our leave-one-out (by "one" we meen "one grid block") cross-validation scheme. 

**Big Woods**:

```{r, eval = run_bw}
# Big Woods
set.seed(76)
bw_cv_grid <- spatialBlock(
  speciesData = bw_growth_df, theRange = 100, verbose = FALSE,
  # Some guess work in figuring this out:
  k = 28, xOffset = 0.5, yOffset = 0
)

# Add foldID to data
bw_growth_df <- bw_growth_df %>% 
  mutate(
    foldID = bw_cv_grid$foldID
  ) 

# Visualize grid. Why does fold 19 repeat?
bw_cv_grid$plots +
  geom_sf(data = bw_growth_df %>% sample_frac(0.2), aes(col=factor(foldID)), size = 0.1)

bw_growth_df <- bw_growth_df %>%
  filter(!foldID %in% c(19, 23, 21, 17, 8, 19)) %>%
  mutate(foldID = as.character(foldID))

# Deliverable
ggplot() +
  geom_sf(data = bw_growth_df, aes(col = foldID, alpha = buffer))

bw_cv_grid_sf <- bw_cv_grid$blocks %>%
  st_as_sf()
```

**SCBI**:

```{r, eval = run_scbi}
# SCBI
scbi_cv_grid <- spatialBlock(
  speciesData = scbi_growth_df, theRange = 100, k = 28, yOffset = 0.9999, verbose = FALSE
)

# Add foldID to data
scbi_growth_df <- scbi_growth_df %>% 
  mutate(
    foldID = scbi_cv_grid$foldID
  )

# Visualize grid
scbi_cv_grid$plots +
  geom_sf(data = scbi_growth_df, aes(col=factor(foldID)), size = 0.1)

scbi_cv_grid_sf <- scbi_cv_grid$blocks %>%
  st_as_sf()
```



### Focal versus competitor trees

Next we create the `focal_vs_comp` data frame which connects each tree in `growth_df` to the trees in its competitive neighborhood range (as defined by `max_dist`). So for example, if `growth_df` consisted of two focal trees with two and three neighbors respectively, `focal_vs_comp` would consist of 5 rows. 

This requires the `growth_df` data frame; `max_dist`, the scalar defining competitive range; `cv_fold_size`, defining the size of the spatial cross-validation blocks; and the `id` variable as inputs.

```{r, eval = run_bw}
# Big Woods
if (!file.exists("focal_vs_comp_bw.Rdata")) {
  tic()
  focal_vs_comp_bw <- bw_growth_df %>% 
    create_focal_vs_comp(max_dist, cv_grid_sf = bw_cv_grid_sf, id = "treeID")
  toc()
  save(focal_vs_comp_bw, file = "focal_vs_comp_bw.Rdata")
} else {
  load("focal_vs_comp_bw.Rdata")
}
```

```{r, eval = run_bw}
# Big Woods
glimpse(focal_vs_comp_bw)
```


**SCBI**:


```{r, eval = run_scbi}
# SCBI
tic()
focal_vs_comp_scbi <- scbi_growth_df %>% 
  create_focal_vs_comp(max_dist, cv_grid_sf = scbi_cv_grid_sf, id = "stemID")
toc()
```

```{r, eval = run_scbi}
# SCBI
glimpse(focal_vs_comp_scbi)
```



### Model fit and prediction

**Big Woods**:

Now we are ready to fit the competition model with `fit_bayesian_model`. This function needs only the `focal_vs_comp` as an input. Other options allow the user to specify prior parameters and run a species identity shuffle (see below).

```{r, eval = run_bw}
# Big Woods
tic()
posterior_param_bw <- focal_vs_comp_bw %>% 
  fit_bayesian_model(prior_param = NULL, run_shuffle = FALSE)
toc()
```

This output has the posterior parameters for the specified competition model. This `posterior_param` output can be used to get predicted growths for each individual (with `predict_bayesain_model`) to test how well the model performs. Or this `posterior_param` output can be plots (either the betas or lambdas) to understand what controls individual growth.

Here we calculate the RMSE

```{r, eval = run_bw}
# Big Woods
predictions <- focal_vs_comp_bw %>%
  predict_bayesian_model(posterior_param = posterior_param_bw) %>%
  right_join(bw_growth_df, by = c("focal_ID" = "treeID"))
predictions %>%
  rmse(truth = growth, estimate = growth_hat) %>%
  pull(.estimate)
```

Now we test whether the identity of the competitor matters. We do this by shuffling the identity of competitors (but not of focal trees or spatial locations or sizes) and fitting the model again. We then compare RMSEs to see whether competitor identity matters to competitive effects

```{r, eval = run_bw}
posterior_param_bw_shuffle <- focal_vs_comp_bw %>%
  fit_bayesian_model(prior_param = NULL, run_shuffle = TRUE)

# b) Make predictions and compute RMSE
predictions_shuffle <- focal_vs_comp_bw %>%
  predict_bayesian_model(posterior_param = posterior_param_bw_shuffle) %>%
  right_join(bw_growth_df, by = c("focal_ID" = "treeID"))
predictions_shuffle %>%
  rmse(truth = growth, estimate = growth_hat) %>%
  pull(.estimate)
```



**SCBI**:

```{r, eval = run_scbi}
# SCBI
tic()
posterior_param_scbi <- focal_vs_comp_scbi %>% 
  fit_bayesian_model(prior_param = NULL, run_shuffle = TRUE)
toc()
```

```{r, eval = run_scbi}
# SCBI
scbi_growth_df <- focal_vs_comp_scbi %>% 
  predict_bayesian_model(posterior_param = posterior_param_scbi) %>% 
  right_join(scbi_growth_df, by = c("focal_ID" = "stemID"))

# Observed vs predicted growth  
ggplot(scbi_growth_df, aes(x = growth, y = growth_hat)) +
  geom_point(size = 0.5, color = rgb(0, 0, 0, 0.25)) +
  stat_smooth(method = 'lm') +
  geom_abline(slope = 1, intercept = 0) +
  coord_fixed() + 
  labs(
    x = "Observed growth in dbh", y = "Predicted growth in dbh", 
    title = "Predicted vs Observed Growth"
  )

reslab <- expression(paste('Residual (cm ',y^{-1},')'))
scbi_growth_df %>% 
  st_as_sf() %>%
  # Need to investigate missingness
  filter(!is.na(growth_hat)) %>% 
  mutate(
    error = growth - growth_hat,
    error_bin = cut_number(error, n = 5), 
    error_compress = ifelse(error < -0.75, -0.75, ifelse(error > 0.75, 0.75, error))
  ) %>% 
  ggplot() + 
  geom_sf(aes(col = error_compress), size = 0.4) + 
  theme_bw() + 
  scale_color_gradient2(
    low = "#ef8a62", mid = "#f7f7f7", high = "#67a9cf", 
    name = reslab,
    breaks = seq(from = -0.75, to = 0.75, by = 0.25),
    labels = c('< -0.75', '-0.5', '0.25', '0', '0.25', '0.5', '> 0.75')) +
  labs(x = "Meter", y = "Meter")
```


### Run spatial cross-validation

**SCBI**

For the above results we fit the model to the entire data set, and then make predictions across the entire data set from that fit. This could lead to overfitting because we are using the training data to also test the model. If model error is spatially correlated this could be a large issue (cite important sources here!). We can use the spatial block structure we defined above to deal with with. The function `run_cv` goes through each fold in the `cv_grid` and fits the model on all the other folds. Then applies that fit to the focal fold. It is a wrapper for `fit_bayesain_model` and `predict_bayesain_model` but fits a seperate model for each fold.

This will fit the model for each fold. On each fold it fits the data for all trees outside of that fold. If you have N folds then `run_cv` will take N-times longer than `fit_bayesain_model`. Here I cheated `run_cv` to fit for just `test = fold 23` and `test = fold 2`. It still fits to all other folds for those two, but only does it twice (rather than 28 times for the SCBI data set). Do this with the arguement `all_folds = FALSE`.

```{r, eval = run_scbi}
# SCBI
tic()
scbi_cv_predict <- focal_vs_comp_scbi %>%
  run_cv(model_specs = scbi_specs, max_dist = max_dist, cv_grid = scbi_cv_grid, all_folds = FALSE)
toc()
```

Running just two folds took 2119 seconds. There are 28 folds. So running everything should take 2119 * 14 / 60 / 60 = 8 hours. 

Then we can compare the results to show that the RMSE for the cross-validated fit is larger than for the none CV fit above. 

```{r, eval = run_scbi}
# SCBI
scbi_cv_predict %>%
  inner_join(scbi_growth_df, by = 'focal_ID', suffix = c('_cv','')) %>%
  summarise(
    rmse_cv = sqrt(mean((growth - growth_hat_cv)^2)),
    rmse = sqrt(mean((growth - growth_hat)^2)), 
    n = n() 
  )
```


**Big Woods**

Big woods is faster because it has fewer trees, but also because we are fitting the model for trait groups (6 groups) rather than species for SCBI (40 species). The lambda matrix is much smaller (6 x 6 versus 40 x 40). Means it fits much faster. 

```{r, eval = run_bw}
# Big Woods
if (!file.exists("bw_cv_predict.Rdata")) {
tic()
bw_cv_predict <- focal_vs_comp_bw %>%
  run_cv(model_specs = bw_specs, max_dist = max_dist, cv_grid = bw_cv_grid)
toc()
  save(bw_cv_predict, file = "bw_cv_predict.Rdata")
} else {
  load("bw_cv_predict.Rdata")
}
```

Big Woods must faster because it is a smaller plot? Mabye also because we are fitting the trait group version (6 spp versus 40 for SCBI).


```{r, eval = run_bw}
# Big Woods
bw_growth_df <- bw_growth_df %>% 
  left_join(bw_cv_predict, by = "focal_ID", suffix = c('', '_cv')) %>% 
  mutate(has_cv = !is.na(growth_hat))

ggplot(bw_growth_df) +
  geom_sf(size = 0.1, aes(col = has_cv))
```

It appears not all trees have a CV predicted value of `growth_hat`. This is because either:

* The tree is part of the buffer
* The tree was dropped in the `create_focal_vs_comp()` phase

```{r, eval = run_bw}
# This code is a hideous sanity check of the two facts above. To delete.
trees_dropped_during_create_focal_vs_comp <- 
  setdiff(unique(bw_growth_df$focal_ID), unique(focal_vs_comp_bw$focal_ID)) %>% 
  sort()
trees_with_no_cv <- bw_growth_df %>% 
  filter(!has_cv) %>% 
  pull(focal_ID) %>% 
  sort()
mean(trees_dropped_during_create_focal_vs_comp == trees_with_no_cv)
```

Let's remove these trees that have no 

```{r, eval = run_bw}
bw_growth_df <- bw_growth_df %>% 
  filter(!is.na(growth_hat))
```

Let's compute summary statistics of our cross-validated errors:

```{r, eval = run_bw}
# Big Woods
bw_growth_df %>% 
  as_tibble() %>% 
  summarise(
    rmse_cv = sqrt(mean((growth - growth_hat)^2)),
    rmse = sqrt(mean((growth - growth_hat)^2)), 
    n = n() 
  )
```

Let's visualize the spatial distribution of our cross-validated errors:

```{r, eval = run_bw}
# Big Woods
bw_growth_df <- bw_growth_df %>%
  mutate(
    error = growth - growth_hat,
    error_bin = cut_number(error, n = 5),
    error_compress = ifelse(error < -0.75, -0.75, ifelse(error > 0.75, 0.75, error))
  )

ggplot(bw_growth_df) +
  geom_sf(aes(col = error_compress), size = 0.4) +
  scale_color_gradient2(
    low = "#ef8a62", mid = "#f7f7f7", high = "#67a9cf", 
    name = expression(paste("Residual (cm ", y^{-1}, ")")),
    breaks = seq(from = -0.75, to = 0.75, by = 0.25),
    labels = c("< -0.75", "-0.5", "0.25", "0", "0.25", "0.5", "> 0.75")
  ) +
  labs(x = "Meter", y = "Meter")
```




### Run permutations

Hey Dave! Start here!

**Repeat earlier code**:

```{r, eval = FALSE}
# Load packages ----------------------------------------------------------------
library(tidyverse)
library(forestecology)
library(snakecase)
library(skimr)
library(sf)
library(sfheaders)
# devtools::install_github("rvalavi/blockCV")
library(blockCV)
library(tictoc)
library(yardstick)
library(viridis)


# Load & preprocess data -------------------------------------------------------
# Read in census data from 2008 & 2014
bw_2008 <- 
  "https://deepblue.lib.umich.edu/data/downloads/z603qx485" %>% 
  read_delim(delim = "\t") %>% 
  mutate(spcode = to_any_case(spcode)) %>%
  select(
    treeID = treeid, stemID = stemtag, sp = spcode, quadrat, gx, gy, dbh, 
    date, codes
  )
bw_2014 <- 
  "https://deepblue.lib.umich.edu/data/downloads/1831ck00f" %>% 
  read_delim(delim = "\t") %>% 
  mutate(spcode = to_any_case(spcode)) %>%
  select(
    treeID = treeid, stemID = stemtag, sp = spcode, quadrat, gx, gy, dbh, 
    date, codes
  )

# Read in grouping classification data
bw_species <- 
  "https://deepblue.lib.umich.edu/data/downloads/000000086" %>% 
  read_delim(delim = "\t") %>% 
  # convert all to snake case:
  mutate_at(c("species", "genus", "family", "idlevel", "spcode"), to_any_case) %>% 
  # join trait group
  left_join(families, by = c("spcode", "family")) %>% 
  mutate(
    sp = str_sub(genus, 1, 2), 
    sp = str_c(sp, str_sub(species, 1, 2)),
    sp = tolower(sp),
    latin = str_c(genus, species, sep = " "),
    latin = to_any_case(latin)
  ) %>% 
  select(sp = spcode, genus, species, latin, family, trait_group)

bw_2008 <- bw_2008 %>%
  left_join(bw_species,by='sp')


# Compute growth ---------------------------------------------------------------
census_df1 <- bw_2008
# we need to filter out the resprouts
census_df2 <- bw_2014 %>% 
  filter(!str_detect(codes, 'R'))
id <- "treeID"

bw_growth_df <- 
  # Merge both censuses and compute growth:
  compute_growth(census_df1, census_df2, id) %>% 
  mutate(sp = to_any_case(sp))



# Define buffers ---------------------------------------------------------------
cv_fold_size <- 100
max_dist <- 7.5

# Study region boundary polygon
bw_boundary <- bigwoods_study_region %>% 
  sf_polygon()

# Buffer polygon
bw_buffer <- bw_boundary %>%
  st_buffer(dist = -max_dist)

# Convert data frame to sf object
bw_growth_df <- bw_growth_df %>% 
  st_as_sf(coords = c("gx", "gy"))

# ID which points are in buffer and which are not
buffer_index <- !st_intersects(bw_growth_df, bw_buffer, sparse = FALSE)
bw_growth_df <- bw_growth_df %>% 
  mutate(buffer = as.vector(buffer_index))


# Define spatial CV folds ------------------------------------------------------
set.seed(76)
bw_cv_grid <- spatialBlock(
  speciesData = bw_growth_df, theRange = 100, verbose = FALSE,
  # Some guess work in figuring this out:
  k = 28, xOffset = 0.5, yOffset = 0
)

# Add foldID to data
bw_growth_df <- bw_growth_df %>% 
  mutate(
    foldID = bw_cv_grid$foldID
  ) 

# Visualize grid. Why does fold 19 repeat?
bw_cv_grid$plots +
  geom_sf(data = bw_growth_df %>% sample_frac(0.2), aes(col=factor(foldID)), size = 0.1)

# Remove weird folds with no trees in them from viz above
bw_growth_df <- bw_growth_df %>%
  filter(!foldID %in% c(19, 23, 21, 17, 8, 19))

# Save original
bw_growth_df_orig <- bw_growth_df
```


**Loop through 4 scenarios of model fitting**:

```{r, eval = FALSE}
# Number of permutation shuffles:
num_shuffle <- 4

# Compute observed RMSE for all models, but only do permutation shuffling for
# models 3,6,9
model_numbers <- c(3)
species_notion_vector <- c("trait_group", "family", "species")

# Save results here
run_time <- rep(0, length(species_notion_vector))
observed_RMSE <- rep(0, length(species_notion_vector))
observed_RMSE_CV <- rep(0, length(species_notion_vector))
shuffle_RMSE <- vector("list", length(species_notion_vector))
shuffle_RMSE_CV <- vector("list", length(species_notion_vector))
filename <- str_c(format(Sys.time(), "%Y-%m-%d"), "_model_comp_tbl_", num_shuffle, "_shuffles.RData")

for(i in 1:length(species_notion_vector)){
  # Start clock
  tic()

  # Modeling and species stuff
  species_notion <- species_notion_vector[i]
  bw_specs <- bw_growth_df %>% 
    get_model_specs(model_number = 3, species_notion = species_notion)

  # Focal vs comp main dataframe for analysis
  focal_vs_comp_bw <- bw_growth_df_orig %>% 
    create_focal_vs_comp(max_dist, model_specs = bw_specs, cv_grid = bw_cv_grid, id = "treeID")
 
  
  # 1. Compute observed test statistic: RMSE with no cross-validation ----
  # Fit model (compute posterior parameters)
  bw_fit_model <- focal_vs_comp_bw %>% 
    fit_bayesian_model(model_specs = bw_specs)
  
  # Make predictions, compute and save RMSE, and reset
  observed_RMSE[i] <- focal_vs_comp_bw %>% 
    predict_bayesian_model(model_specs = bw_specs, posterior_param = bw_fit_model) %>% 
    right_join(bw_growth_df_orig, by = c("focal_ID" = "treeID")) %>%
    rmse(truth = growth, estimate = growth_hat) %>%
    pull(.estimate)

  
  # 2. Compute observed test statistic: RMSE with cross-validation ----
  observed_RMSE_CV[i] <- focal_vs_comp_bw %>%
    run_cv(model_specs = bw_specs, max_dist = max_dist, cv_grid = bw_cv_grid) %>% 
    right_join(bw_growth_df_orig, by = c("focal_ID" = "treeID")) %>%
    rmse(truth = growth, estimate = growth_hat) %>%
    pull(.estimate)


  # 3. Permutation distribution: RMSE with no cross-validation ----
  # Only do permutation shuffling for models 3, 6, 9
  # Compute num_shuffle permutation test statistics
  shuffle_RMSE[[i]] <- numeric(length = num_shuffle)
  
  for(j in 1:num_shuffle){
    # Fit model (compute posterior parameters) with shuffling
    bw_fit_model_shuffle <- focal_vs_comp_bw %>% 
      fit_bayesian_model(model_specs = bw_specs, run_shuffle = TRUE)
    
    # Make predictions, compute and save RMSE, and reset
    shuffle_RMSE[[i]][j] <- focal_vs_comp_bw %>% 
      predict_bayesian_model(model_specs = bw_specs, posterior_param = bw_fit_model_shuffle) %>% 
      right_join(bw_growth_df_orig, by = c("focal_ID" = "treeID")) %>%
      rmse(truth = growth, estimate = growth_hat) %>%
      pull(.estimate)
  }

  
  # 4. Permutation distribution: RMSE with cross-validation ----
  # Compute num_shuffle permutation test statistics
  shuffle_RMSE_CV[[i]] <- numeric(length = num_shuffle)
  
  # Compute num_shuffle permutation test statistics
  for(j in 1:num_shuffle){
    # Compute and save RMSE, and reset
    shuffle_RMSE_CV[[i]][j] <- focal_vs_comp_bw %>%
      run_cv(model_specs = bw_specs, max_dist = max_dist, cv_grid = bw_cv_grid, run_shuffle = TRUE) %>% 
      right_join(bw_growth_df_orig, by = c("focal_ID" = "treeID")) %>%
      rmse(truth = growth, estimate = growth_hat) %>%
      pull(.estimate)
    
    # Status update
    str_c("notion of species: ", species_notion_vector[i], ", shuffle with permutation ", j) %>% print()
  }
  
  
  # 5. Stop clock
  clock <- toc(quiet = TRUE)
  run_time[i] <- clock$toc - clock$tic
  
  
  # 6. Save
  model_comp_tbl <- tibble(
    species_notion = species_notion_vector,
    run_time = run_time,
    observed_RMSE = observed_RMSE,
    observed_RMSE_CV = observed_RMSE_CV,
    shuffle_RMSE = shuffle_RMSE,
    shuffle_RMSE_CV = shuffle_RMSE_CV,
  )
  save(model_comp_tbl, file = filename)
}
```

**Plot results**:

```{r, eval=TRUE}
load("2020-06-25_model_comp_tbl_49_shuffles.RData")
model_comp <- bind_rows(
  model_comp_tbl %>% select(species_notion, run_time, observed = observed_RMSE, shuffle = shuffle_RMSE) %>% mutate(CV = FALSE),
  model_comp_tbl %>% select(species_notion, run_time, observed = observed_RMSE_CV, shuffle = shuffle_RMSE_CV) %>% mutate(CV = TRUE)
) %>%
  gather(type, RMSE, -c(species_notion, run_time, CV)) %>%
  mutate(
    species_notion = case_when(
      species_notion == "trait_group" ~ "1. Trait-based (6): lambda = 6 x 6",
      species_notion == "family" ~ "2. Phylogenetic family (20): lambda = 20 x 20",
      species_notion == "species" ~ "3. Actual species (36): lambda = 36 x 36"
    )
  ) %>% 
  filter(species_notion != "3. Actual species (36): lambda = 36 x 36")

model_comp_observed <- model_comp %>%
  filter(type == "observed") %>%
  unnest(cols = c(RMSE))
model_comp_shuffle <- model_comp %>%
  filter(type == "shuffle") %>%
  unnest(cols = c(RMSE))

xlab <- expression(paste('RMSE (cm ',y^{-1},')'))

ggplot() +
  geom_vline(data = model_comp_observed, aes(xintercept = RMSE, col = CV), linetype = "dashed", show.legend = F) +
  geom_histogram(data = model_comp_shuffle, aes(x = RMSE, fill = CV), bins = 200) +
  labs(fill = "Cross-validated?", x = xlab) +
  facet_wrap(~species_notion, ncol = 1) +
  scale_color_viridis(discrete = TRUE, option = "D")+
  scale_fill_viridis(discrete = TRUE)
```






### Visualize posterior distributions

* Plot posterior distributions of all parameters

We might be interested in the posterior distributions of parameters. The betas tell us about how fast each species grows and how this depends on DBH. The lambdas, which are often of more interest, are the species-specific competition coefficents. The full lambda matrix gives competition strength between species. There is a rich literature how this matrxi (cite). 

Because of the strucutre of the `bw_fit_model` object we cannot simply draw these curves based on the posterior distribution. `bw_fit_model` gives the parameteres *compared* to a baseline. This is often not of interest. So to display these parameters as we care about them we have to sample from the baseline distrubiton and from the comparison one to get the posterior distribution of interest. 

```{r, eval = run_bw}

# plot_beta0(bw_fit_model)

```



